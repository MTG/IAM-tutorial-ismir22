{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(music-segmentation)=\n",
    "# Music segmentation\n",
    "We have seen in the musicological introduction that we may come across different formats of [Carnatic](carnatic-formats) and Hindustani performances. These must be taken very much into account when designing strategies to segment the different sections in these musical pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Importing compiam to the project\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first list the available tools for music segmentation in `compiam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiam.structure.segmentation.list_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dhrupad Bandish segmentation\n",
    "\n",
    "In this section we will showcase a tool that attempts to identify, through the use of rhythmic features, the different sections in a Dhrupad Bandish performances {cite}`rohit_2020`, one of the main formats in Hindustani music. As seen in the [documentation](https://mtg.github.io/compIAM/source/structure.html#dhrupad-bandish-segmentation), this segmentation model is based on PyTorch. Therefore, we proceed to install ``torch``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install torch==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool may be accessed from the ``structure.segmentation``, however, the tool name has an ``*`` appended, therefore we can use the wrapper for models to rapidly initialize it with the pre-trained weights loaded.\n",
    "\n",
    "```{tip}\n",
    "Get the correct code for the wrapper by running ``compiam.list_models()``.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = compiam.load_model(\"structure:dhrupad-bandish-segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the documentation we observe that this model includes quite a number of attributes, and particularly we observe two of them that are interesting:\n",
    "* ``mode``\n",
    "* ``fold``\n",
    "\n",
    "These attributes are important because define the training pipeline that has been used and therefore, a different mode of operating with this model. ``mode`` has options: *net*, *voc*, or *pakh*, which indicate the source for  surface tempo multiple (s.t.m.) estimation. *net* mode is for input mixture signal, *voc* is for clean or source-separated singing voice recordings, and *pakh* for pakhawaj tracks (pakhawaj is a percussion instrument from Northern India). ``fold`` is basically an integer indicating with validation fold we do consider for training.\n",
    "\n",
    "These configuration variables are loaded by default as ``net`` and ``0`` respectively, however these may be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs.update_mode(mode=\"voc\")\n",
    "dbs.update_fold(fold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, the ``mode`` and ``fold`` have been updated and consequently, the class has automatically loaded the model weights corresponding to ``mode=voc`` and ``fold=1``.\n",
    "\n",
    "```{note}\n",
    "Typically in `compiam`, importing a model from the corresponding module or initializing it using the wrapper, can make an important difference on how the loaded instance works. Generally speaking, if you use the wrapper you will probably be only interested in running inference. If your goal is to train or deep-dive into a particular model, you should avoid the use of the model wrapper and start from a clean model instance.\n",
    "```\n",
    "\n",
    "Let's now run prediction on an input file. Our mode now is ``voc``, therefore the model expects a clean or source separated vocal signal. Isolated singing voice signals are not commonly available for the case of Carnatic and Hindustani music. We will use a state-of-the-art and out-of-the-box model, [`Spleeter`](https://github.com/deezer/spleeter), to try to separate the singing voice from the accompaniment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install spleeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now directly download the pre-trained models for `Spleeter`, and use these for inference in this walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the \"tarfile\" module\n",
    "import tarfile\n",
    "\n",
    "# open file\n",
    "file = tarfile.open(\"2stems.tar.gz\")\n",
    "os.mkdir(\"pretrained_models/\")\n",
    "\n",
    "# extracting file\n",
    "file.extractall(\n",
    "    os.path.join(\"pretrained_models\", \"2stems\")\n",
    ")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spleeter` is based on `TensorFlow`. We disable the GPU usage and the `TensorFlow` related warnings just like we did in the [pitch extraction walkthrough](melody-extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling tensorflow warnings and debugging info\n",
    "import os \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "\n",
    "# Importing tensorflow and disabling GPU usage\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now load the `Spleeter` separator, which will automatically load the pre-trained weights for the model. We will use the ``2:stems`` model, which has been trained to separate vocals and accompaniment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spleeter.separator import Separator\n",
    "\n",
    "# Load default 2-stem spleeter separation\n",
    "separator = Separator('spleeter:2stems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Separator` class in `Spleeter` has a method to directly separate the singing voice from an audio file, and the prediction is stored in a given output folder. Let's use this method and get a source separated version of our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating!\n",
    "separator.separate_to_file(\n",
    "    os.path.join(\n",
    "        \"..\", \"audio\", \"mir_datasets\", \"CMR_full_dataset_1.0\",\n",
    "        \"audio\", \"10001_05_Thunga_Theera_Virajam.wav\"\n",
    "    ),\n",
    "    os.path.join(\"..\", \"audio\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../audio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs.predict_stm(path_to_file=os.path.join(\"..\", \"audio\", \"test_1_vocals.wav\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
