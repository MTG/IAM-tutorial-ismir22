{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(meter-analysis)=\n",
    "# Meter analysis\n",
    "In the musical introduction, the main rhythmic characteristics of Carnatic and Hindustani traditions are provided. The presented hierarchical form of a tala is targeted in {cite}`srinivasamurthy_rhythm_2014`, using a supervised approach on top of spectral hand-crafted features that capture tempo and onsets in the signal. Bayesian models are found to be the most capable of capture the rhythmic cycles in music signals, first applied in the form of particle filters {cite}`srinivasamurthy_rhythm_2015` for an efficient inference, showing competetitive results on Carnatic recordings. The said approach is futher extended to a generalized model to capture long cycles {cite}`srinivasamurthy_rhythm_2016`, being this very convenient for rhythm tracking in Hindustani Music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Installing (if not) and importing compiam to the project\n",
    "import importlib.util\n",
    "if importlib.util.find_spec('compiam') is None:\n",
    "    ## Bear in mind this will only run in a jupyter notebook / Collab session\n",
    "    %pip install compiam\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "from pprint import pprint\n",
    "import IPython.display as ipd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(cmr-hmr)=\n",
    "## Carnatic and Hindustani Music Rhyhtm datasets\n",
    "That is a precise moment to introduce the (CompMusic) [Carnatic](https://zenodo.org/record/1264394) and [Hindustani](https://zenodo.org/record/1264742) Rhythm Datasets. These datasets, which include audio recordings, musically-relevant metadata, and beat and meter annotations, can be downloaded from Zenodo under request and used through the `mirdata` dataloaders available from [release 0.3.7](https://github.com/mir-dataset-loaders/mirdata/releases/tag/0.3.7). These dataset present useful for the metrical and beat estimation research on Indian Art Music.\n",
    "\n",
    "Let's initialise an instance of these datasets and browse through the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmr = compiam.load_dataset(\n",
    "    \"compmusic_carnatic_rhythm\",\n",
    "    data_home=os.path.join(\"../audio/mir_datasets\"),\n",
    "    version=\"full_dataset\"\n",
    ")\n",
    "cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For showcasing purposes, we include a single-track version of the (CompMusic) Carnatic Rhythm Dataset within the materials of this tutorial. This dataset is private, but can be requested and downloaded for research purposes.\n",
    "\n",
    "Reading through the dataset details we observe the available data for the tracks in the dataloader. We will load the tracks and select the specific track we have selected for tutoring purposes. Bear in mind that you can access the list of identifiers in the dataloader with the ``.track_ids`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = cmr.load_tracks()[\"10001\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the available annotations for this example track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``BeatData`` is an annotation type that is used in `mirata` to store information related to rhythmic beats. We will print the time-steps for the first 20 annotated beats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.times[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also observe the actual magnitude of these annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.time_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have confirmed that the beats are annotated in `s`, seconds. We can also observe the positons in the cycle (if available) that each of the beats occupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.positions[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mirdata` annotations have been created aiming at providing standardized and useful data structures for MIR-related annotations. In this example we showcase the use of ``BeatData``, but many more annotation types are included in `mirdata`. Make sure to [check them out](https://mirdata.readthedocs.io/en/latest/source/mirdata.html#annotations)!.\n",
    "\n",
    "Let's now observe how the meter annotations looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, no tools to track the beats or the meter in Indian Art Music performances are available in `compiam` as of now. We are looking forward to covering these tasks in the near future!\n",
    "\n",
    "Nonetheless, in this section we would like to showcase a relevant tool to track the [aksharas](carnatic-rhythm) in a music recording.\n",
    "\n",
    "(akshara-pulse-tracker)=\n",
    "## Akshara pulse tracker\n",
    "\n",
    "Let's first listen to the audio example we are going to be using to showcase this tool. We will just load the first 30 seconds (we can just slice the audio array from 0 to $f_{s} * sec$, being $sec$ the integer  number of seconds to get)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listen to 30 seconds of the input audio\n",
    "track = cmr.load_tracks()[\"10001\"]\n",
    "audio, sr = track.audio\n",
    "ipd.Audio(\n",
    "    data=audio[:10*sr],\n",
    "    rate=sr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import the onset detection tool from ``compiam.rhythm.meter``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the tool\n",
    "from compiam.rhythm.meter import AksharaPulseTracker\n",
    "\n",
    "# Let's initialize an instance of the tool\n",
    "apt = AksharaPulseTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise the other extractors and models in `compiam` (if not indicated otherwise), the method for inference takes an audio path as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "predicted_aksharas = apt.extract(track.audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the first 20 onset estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_aksharas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the ``compiam.visualisation`` to plot the input audio with the annotations on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.visualisation.audio import plot_waveform\n",
    "help(plot_waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the ``labels`` input variable in ``plot_waveforms`` needs to be a dict with the following format: ``{time-step: label}``, while our estimation is basically a list of pulses. Therefore, we first need to convert the prediction into a dictionary. We take the predicted beats, convert these into a dictionary, and plot them on top of the waveform of the input signal again, in order to compare between the estimation and the ground-truth above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_beats_dict = {\n",
    "    time_step: idx for idx, time_step in enumerate(predicted_aksharas)\n",
    "}\n",
    "\n",
    "# And we plot!\n",
    "plot_waveform(\n",
    "    input_data=track.audio_path,\n",
    "    t1=0,\n",
    "    t2=4,\n",
    "    labels=predicted_beats_dict,\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
