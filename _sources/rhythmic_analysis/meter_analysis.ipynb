{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(meter-analysis)=\n",
    "# Meter analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Importing compiam to the project\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "from pprint import pprint\n",
    "import IPython.display as ipd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(cmr-hmr)=\n",
    "## Carnatic and Hindustani Music Rhyhtm datasets\n",
    "That is a precise moment to introduce the (CompMusic) [Carnatic](https://zenodo.org/record/1264394) and [Hindustani](https://zenodo.org/record/1264742) Rhythm Datasets. These datasets, which include audio recordings, musically-relevant metadata, and beat and meter annotations, can be downloaded from Zenodo under request and used through the `mirdata` dataloaders available from [release 0.3.7](https://github.com/mir-dataset-loaders/mirdata/releases/tag/0.3.7).\n",
    "\n",
    "Let's initialise an instance of these datasets and browse through the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmr = compiam.load_dataset(\n",
    "    \"compmusic_carnatic_rhythm\",\n",
    "    data_home=os.path.join(\"../audio/mir_datasets\"),\n",
    "    version=\"full_dataset\"\n",
    ")\n",
    "cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For showcasing purposes, we include a single-track version of the (CompMusic) Carnatic Rhythm Dataset within the materials of this tutorial. This dataset is private, but can be requested and downloaded for research purposes.\n",
    "\n",
    "Reading through the dataset details we observe the available data for the tracks in the dataloader. We will load the tracks and select the specific track we have selected for tutoring purposes. Bear in mind that you can access the list of identifiers in the dataloader with the ``.track_ids`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = cmr.load_tracks()[\"10001\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the available annotations for this example track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``BeatData`` is an annotation type that is used in `mirata` to store information related to rhythmic beats. We will print the time-steps for the first 20 annotated beats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.times[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also observe the actual magnitude of these annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.time_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have confirmed that the beats are annotated in `s`, seconds. We can also observe the positons in the cycle (if available) that each of the beats occupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.positions[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mirdata` annotations have been created aiming at providing standardized and useful data structures for MIR-related annotations. In this example we showcase the use of ``BeatData``, but many more annotation types are included in `mirdata`. Make sure to [check them out](https://mirdata.readthedocs.io/en/latest/source/mirdata.html#annotations)!.\n",
    "\n",
    "Let's now observe how the meter annotations looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a list of works for metrical analysis in Indian Art Music are available {cite}`srinivasamurthy_rhythm_2014, srinivasamurthy_rhythm_2015, srinivasamurthy_rhythm_2016, srinivasamurthy_rhythm_2017a`, no tools to track the beats or the meter in Indian Art Music performances are available in `compiam` as of now. We are looking forward to covering these tasks in the near future!\n",
    "\n",
    "Nonetheless, in this section we would like to showcase a relevant tool to track the [aksharas]() in a music recording.\n",
    "\n",
    "(akshara-pulse-tracker)=\n",
    "## Akshara pulse tracker\n",
    "\n",
    "Let's first listen to the audio example we are going to be using to showcase this tool. We will just load the first 30 seconds (we can just slice the audio array from 0 to $f_{s} * sec$, being $sec$ the integer  number of seconds to get)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listen to 30 seconds of the input audio\n",
    "audio, sr = track.audio\n",
    "ipd.Audio(\n",
    "    data=audio[:10*sr],\n",
    "    rate=sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import the onset detection tool from ``compiam.rhythm.meter``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the tool\n",
    "from compiam.rhythm.meter import AksharaPulseTracker\n",
    "\n",
    "# Let's initialize an instance of the tool\n",
    "apt = AksharaPulseTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise the other extractors and models in `compiam` (if not indicated otherwise), the method for inference takes an audio path as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "predicted_beats = apt.extract(track.audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the first 20 onset estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_beats[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the ``compiam.visualisation`` to plot the input audio with the annotations on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.visualisation.audio import plot_waveform\n",
    "help(plot_waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first plot the audio waveform of the example track with the ground-truth annotations on top. We observe that the ``labels`` input variable in ``plot_waveforms`` needs to be a dict with the following format: ``{time-step: label}``, while our estimation is basically a list of pulses. Therefore, we first need to convert the prediction into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from BeatData to {time-step: value} dict for plotting\n",
    "ground_truth_beats = {\n",
    "    time_step: position for time_step, position in \\\n",
    "        zip(track.beats.times, track.beats.positions)\n",
    "}\n",
    "\n",
    "# We then plot the first 5 seconds\n",
    "plot_waveform(\n",
    "    file_path=track.audio_path,\n",
    "    t1=0,\n",
    "    t2=5,\n",
    "    labels=ground_truth_beats,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take the predicted beats, convert these into a dictionary as well, and plot them on top of the waveform of the input signal again, in order to compare between the estimation and the ground-truth above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_beats_dict = {\n",
    "    time_step: idx for idx, time_step in enumerate(predicted_beats)\n",
    "}\n",
    "\n",
    "# And we plot!\n",
    "plot_waveform(\n",
    "    file_path=track.audio_path,\n",
    "    t1=0,\n",
    "    t2=4,\n",
    "    labels=predicted_beats_dict,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `Essentia` includes a pretty cool method to create audio marks at given onsets on top of a particular input audio. Let's listen how to detected onsets sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install essentia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing `Essentia` in case it is not, we can define an ``AudioOnsetsMarker`` instance to mark the input audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import essentia\n",
    "import numpy as np\n",
    "import essentia.standard as estd\n",
    "print(np.shape(predicted_beats))\n",
    "marker = estd.AudioOnsetsMarker(onsets=essentia.array(predicted_beats), type=\"beep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the marked audio and listen both unmarked and marked audio signals! We only display the first 30 seconds of the entire recordings, which is notably long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\n",
    "    data=audio[:10*sr],\n",
    "    rate=sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_audio = marker(audio[:10*sr])\n",
    "\n",
    "ipd.Audio(\n",
    "    data=marked_audio[:10*sr],\n",
    "    rate=sr,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
