{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(melody-extraction)=\n",
    "# Pitch extraction\n",
    "\n",
    "As seen in the melodic introduction, predominant and vocal pitch is a very relevant feature to tackle the melodic analysis of Carnatic and Hindustani Music. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Importing compiam to the project\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first print out the available tools we do have available to extract the pitch from Indian Art Music recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(compiam.melody.pitch_extraction.list_tools())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an Indian Art Music context, this task has been mainly approached through *heuristic-based approaches* {cite}`rao_pitch_2010, salamon_pitch_2012`, which have been used yet in recent years.\n",
    "\n",
    "Let's extract the pitch from an audio sample using Melodia {cite}`salamon_pitch_2012`. We first need to install `essentia`, which is the optional dependency required to load this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install essentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Importing and initializing a melodia instance\n",
    "from compiam.melody.pitch_extraction import Melodia\n",
    "melodia = Melodia()  \n",
    "\n",
    "# Running extraction for an example track\n",
    "melodia_pitch_track = melodia.extract(\"../audio/testing_samples/test_1.wav\")\n",
    "\n",
    "print(\"Shape of the output pitch:\", np.shape(melodia_pitch_track))\n",
    "pprint(\"First 5 time-stamps:\", melodia_pitch_track[:5, 0])\n",
    "pprint(\"Last 5 pitch values:\", melodia_pitch_track[:-5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melodia has been found, in the original paper experiments and also in the [MIREX campaign](https://nema.lis.illinois.edu/nema_out/mirex2011/results/ame/indian08/sg1results.html), to decently work on Indian Art Music samples. However, recent DL-based models have claimed the state-of-the-art for the task of pitch extraction. \n",
    "\n",
    "**Maybe we can use a Carnatic-trained version of one of these models to extract the pitch?** Let's now import a DL model that learns to automatically extract the predominant melody from audio recordings. In the documentation we observe that this model is based on `tensorflow`, therefore we must install this dependency before importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow==2.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.melody.pitch_extraction import FTANetCarnatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first deactivate the GPU usage, since we assume no CUDA-capable GPU is available in most of the cases. We import `tensorflow` and set the visible GPU devices to none.\n",
    "\n",
    "```{note}\n",
    "If you have an available GPU to allocate the model, get the index of the GPU (probably 0 if you have only a single instance) and change ``tf.config.set_visible_devices([], \"GPU\")`` for ``os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"``\n",
    "```\n",
    "\n",
    "We also disable the `tensorflow` warnings in order to keep the tutorial clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling tensorflow warnings and debugging info\n",
    "import os \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "\n",
    "# Importing tensorflow and disabling GPU usage\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ths case, we are only interested in inference. Therefore, we might be able to load FTANet as an already trained model. For that, let's print the models with available weights to load in `compiam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(compiam.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cool! FTANet tuned to Carnatic Music is there.** Therefore, let's load it and run inference on an example track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an FTANet instance\n",
    "ftanet_carnatic = compiam.load_model(\"melody:ftanet-carnatic\")\n",
    "\n",
    "# Predict!\n",
    "ftanet_pitch_track = ftanet_carnatic.predict(\"../audio/testing_samples/test_1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the extracted pitch tracks on top of the spectrogram of the input signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y, sr = librosa.load(\"../audio/testing_samples/test_1.wav\")\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax)\n",
    "plt.plot(melodia_pitch_track[:, 1], color=\"white\")\n",
    "plt.plot(ftanet_pitch_track[:, 1], color=\"orange\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
