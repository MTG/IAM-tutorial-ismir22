{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1420797e",
   "metadata": {},
   "source": [
    "(melodic-pattern-discovery)=\n",
    "# Melodic pattern dicovery\n",
    "\n",
    "Melodic pattern discovery is a core task within the melodic analysis of Indian Art Music, most of the approaches building on top of time-series of pitch {cite}`ishwar_patterns_2013, rao_patterns_2014, gulati_patterns_2015, gulati_patterns_2016, gulati_patterns_2016_2`. In fact, {cite}`velankar_patterns_2018` reviews the melodic recognition task in Carnatic Music and concludes that using pitch data drives to better performance. On the same line, a musically-relevant statistical analysis of melodic patterns is proposed in {cite}`viraraghavan_patterns_2017`, aiming at providing a more handy representation of this important aspect for Carnatic and Hindustani Music. \n",
    "\n",
    "More recently, the task of melodic pattern discovery has been approached taking advantage of DL techniques by combining the learnt features from a complex autoencoder and an attention-based vocal pitch extraction model {cite}`nuttall_patterns_2022b`. Multiple design choices of this entire process are informed by tradition characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installing (if not) and importing compiam to the project\n",
    "import importlib.util\n",
    "if importlib.util.find_spec('compiam') is None:\n",
    "    ## Bear in mind this will only run in a jupyter notebook / Collab session\n",
    "    %pip install compiam\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a79ee",
   "metadata": {},
   "source": [
    "# Sañcara search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1604f3f-06f2-487a-9d30-5f25d5b24c5f",
   "metadata": {},
   "source": [
    "In this walkthrough we demonstrate the task of repeated melodic motif identification in Carnatic Music. The methodologies used are presented in {cite}`nuttall_patterns_2022b` and {cite}`plaja_pitch_2023` for which compIAM repository serves as the most up to date and well-maintained implementation.\n",
    "\n",
    "In the [compIAM documentation](https://mtg.github.io/compIAM/source/melody.html#cae-carnatic-wrapper) we can see that the tool we showcase in this page has ``torch`` as a dependency for the DL models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656b7c4",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b5b9a",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "This notebook works with a performance from the [Saraga Carnatic Dataset](https://mtg.github.io/saraga/). This performance audio is not provided alongside this notebook as part of the compIAM package but can be downloaded manually following the instructions in the **Access** section of the provided link. Although we encourage the reader to try with their own Carnatic performance audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee4b442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = os.path.join(\"..\", \"audio\", \"pattern_finding\", \"Koti Janmani.multitrack-vocal.mp3\")  # path to audio\n",
    "raga = 'ritigowla'  # carnatic raga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f216b",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "<a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd4bfb",
   "metadata": {},
   "source": [
    "## 2. Pitch processing\n",
    "### 2.1 Predominant pitch extraction\n",
    "Owing to the the coarticulation (merging) of svaras through gamakas, musically salient units in Carnatic Music are often better characterised by continuous time series pitch data rather than transcription to symbolic notation {cite}`nuttall_patterns_2022a, pearson_coarticulation_2016`.\n",
    "\n",
    "However, Carnatic Music constitutes a difficult case for vocal pitch extraction – although performances place strong emphasis on a monophonic melodic line from the soloist singer, heterophonic melodic elements also occur, for example from the accompanying violinist who shadows the melody of the soloist often at a lag and with variation. In addition, there are the sounds of the tanpura (plucked lute that creates an oscillating drone) and pitched percussion instruments {cite}`plaja_pitch_2023`.\n",
    "\n",
    "Here we use a pretrained FTA-Net model for the task. This is provided with the compIAM package via the `compiam.load_model()` function. The model is an attention-based network that leverages and fuses information from the frequency and periodicity domains to capture the correct pitch values for the predominant source. It learns to focus on this source by using an additional branch that helps reduce the false alarm rate (detecting pitch values that do not correspond to the source we target) {cite}`yu_pitch_2021`.\n",
    "\n",
    "This FTANet instance is trained on the [Saraga Carnatic Melody Synth dataset (SMCS)](https://zenodo.org/record/5553925), a dataset including more than 1000 minutes of time-aligned and continuous vocal melody annotations for the Carnatic music tradition {cite}`plaja_pitch_2023`. See also the [FTANet-Carnatic walkthrough](melody-extraction-ftanet) in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee11a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftanet = compiam.load_model(\"melody:ftanet-carnatic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23ca14",
   "metadata": {},
   "source": [
    "Extract predominant pitch track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a0d59b",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "pitch_track = ftanet.predict(audio_path)\n",
    "pitch = pitch_track[:,1] # Pitch in Hz\n",
    "time  = pitch_track[:,0] # Time in seconds\n",
    "timestep  = time[2]-time[1] # time in seconds between elements of pitch track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202bb03",
   "metadata": {},
   "source": [
    "We can interpolate small silences to account for minor errors in the pitch exrraction process, typically caused by glottal sounds and sudden decrease of pitch salience in gamakas {cite}`nuttall_patterns_2022a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6795f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.utils.pitch import interpolate_below_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15fd7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch = interpolate_below_length(\n",
    "    pitch, # track to interpolate\n",
    "    0, # value to interpolate \n",
    "    350*0.001/timestep # maximum gap in number sequence elements to interpolate for\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f32e2",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8e533",
   "metadata": {},
   "source": [
    "### 2.2 Visualising predominant pitch\n",
    "We can plot our pitch track using the visualisation tools in `compiam.visualisation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2284963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.visualisation.pitch import plot_pitch, flush_matplotlib\n",
    "from compiam.utils import ipy_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c642b11",
   "metadata": {},
   "source": [
    "We want to accompany pitch plots with audio so load raw audio too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33efbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the audio also\n",
    "import librosa\n",
    "sr = 44100 # sampling_rate\n",
    "audio, _ = librosa.load(audio_path,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0018d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 304 # in seconds\n",
    "t2 = 324 # in seconds\n",
    "t1s = round(t1/timestep) # in sequence elements\n",
    "t2s = round(t2/timestep) # in sequence elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c880756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_pitch = pitch[t1s:t2s]\n",
    "this_time = time[t1s:t2s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(this_pitch, this_time, title='Excerpt of Koti Jamani by The Akkarai Sisters')\n",
    "ipy_audio(audio, t1, t2, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a80ea6",
   "metadata": {},
   "source": [
    "The plot is OK but we could definitely make it more interpretable. First of all, since silences are represented as 0Hz in our predominant pitch track, the interpolationm between points creates large drops to 0 for these regions. We can pass a binary mask to `plot_pitch` to tell it not to plot certain areas. In this case we want this mask to be 1 when there is non-silence (non-zero) and 1 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04875aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_mask = this_pitch==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(this_pitch, this_time, mask=silence_mask, title='Excerpt of Koti Jamani by The Akkarai Sisters')\n",
    "ipy_audio(audio, t1, t2, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd585f0",
   "metadata": {},
   "source": [
    "Pitch is often thought about in terms of it's relationship to the tonic (Sa), which we can quantify using cents. A tonic identifier is provided in `compiam.melody`, or alternatively the tonic may be provided precomputed in the Saraga Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7bec24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.melody import tonic_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "296e0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tonicExt = tonic_identification.TonicIndianMultiPitch()\n",
    "tonic = 195.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cf39c",
   "metadata": {},
   "source": [
    "Passing this tonic to `plot_pitch` converts the plotted curve to cents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(this_pitch, this_time, mask=silence_mask, tonic=tonic, cents=True, title='Excerpt of Koti Jamani by The Akkarai Sisters')\n",
    "ipy_audio(audio, t1, t2, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8745589",
   "metadata": {},
   "source": [
    "Finally we go one step further and replace the cents ticks on the y-axis with the expected svaras for this raga, plotted at their theoretical pitch positions on an equal tempered scale. \n",
    "\n",
    "For a growing list of ragas, these theoretical svara:pitch mappings are available from `compiam.utils.get_svara_pitch_carnatic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9734697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.utils import get_svara_pitch_carnatic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1589a2a",
   "metadata": {},
   "source": [
    "It is not uncommon to observe variations in how raga names (and more broadly, Indian Art Music terms) are transliterated from their original Indian language to latin-script. This makes it difficult to create key value mappings using these terms. If an unknown raga is passed to `get_svara_pitch_carnatic`, a list of closest matches are suggested as alternatives (if any exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fd30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_svara_pitch_carnatic(raga)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37a0ae",
   "metadata": {},
   "source": [
    "Passing the tonic returns these pitches in Hz, else they are returned in cents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f073ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svara_pitch = get_svara_pitch_carnatic('ritigaula', tonic=tonic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90773cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "svara_pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75ed35",
   "metadata": {},
   "source": [
    "`plot_pitch` accepts alternative y ticks in the format `y-values:label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5367e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(this_pitch, this_time, mask=silence_mask, tonic=tonic, yticks_dict=svara_pitch, cents=True, title='Excerpt of Koti Jamani by The Akkarai Sisters')\n",
    "ipy_audio(audio, t1, t2, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d04a9",
   "metadata": {},
   "source": [
    "It should be noted that Carnatic music does not use the equal tempered scale, and indeed that the same svara position, e.g., R2, may be deliberately placed slightly differently in different ragas. Therefore, any deviation here from those theoretical pitch positions does not indicate an error in intonation, but more likely reflects correct musical practice in the style or slight errors in the automated calculation of the tonic.\n",
    "<a id='3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13556c4",
   "metadata": {},
   "source": [
    "<a id='3.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16813d0f",
   "metadata": {},
   "source": [
    "## 3. Melodic pattern analysis\n",
    "Another important feature of the style is the structural and expressive significance of motifs and phrases known as sañcāras, which can be defined as coherent segments of melodic movement that follow the grammar of the rāga {cite}`pesch_oxford_2009, viswanathan_analysis_1977`. These melodic patterns are the means through which the character of the rāga is expressed and form the basis of various improvisatory and compositional formats in the style {cite}`ishwar_patterns_2013, viswanathan_analysis_1977`. There exists no definitive lists of all possible sañcāras in each rāga, rather the body of existing compositions and the living oral tradition of rāga performance act as repositories for that knowledge {cite}`nuttall_patterns_2022b`.\n",
    "\n",
    "Certain characteristics of Carnatic Music (some of which it shares with many traditions around the world) make the task of identifying repeated motifs and phrases in Carnatic music performances particuarly difficult, particularly in how certain motifs and phrases are often performed across occurrences with variations in tempo, pitch and ornamentation.\n",
    "\n",
    "We return to Koti Janmani for the pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "441ba89d-ca33-44b5-ad2c-bf1d112233e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.utils.pitch import pitch_seq_to_cents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca975960",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_cents = pitch_seq_to_cents(pitch, tonic=tonic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6d63b",
   "metadata": {},
   "source": [
    "\n",
    "<a id='4.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2ab26",
   "metadata": {},
   "source": [
    "### 3.1 Melodic feature extraction\n",
    "To identify repeated melodic patterns in our audio we use melodic features computed for windows across the entirety of the track and use the pairwise similarity between these to identify regions of consistent high-similarity... in theory corresponding to melodic repetitions.\n",
    "\n",
    "In principle, any set of features can be used in this section. The extent to which these features capture the aspects of melody we care about will define their usefulness for the task.\n",
    "\n",
    "Here we use melodic features extracted using a Complex Autoencoder (CAE). Mapping signals onto complex basis functions learnt by the CAE results in a magnitude space that has proven to achieve state-of-the-art results in repeared section discovery for audio {cite}`lattner_cae_2019`.\n",
    "\n",
    "The CAE we use here is provided via `compiam.load_models` and has been trained on the entirety of the Carnatic Saraga dataset for which we have multitrack recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca12aeb",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Pattern Extraction for a Given Audio\n",
    "from compiam import load_model\n",
    "\n",
    "# Feature Extraction\n",
    "# CAE features\n",
    "cae = load_model(\"melody:cae-carnatic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae256be6",
   "metadata": {},
   "source": [
    "Extracting features across the entirety of our chosen audio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns magnitude and phase\n",
    "ampl, _ = cae.extract_features(audio_path)\n",
    "ampl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40e9f1",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023db74f",
   "metadata": {},
   "source": [
    "### 3.2 Self similarity\n",
    "We want to compute the pairwsie self similarity between all combinations of features. With some Carnatic performances lasting up to over an hour, this can often be an expensive computation. There are many regions of the track that we are not interested in (such as silence), or that serve as plausible segmentation points for repeated melodic patterns (such as long periods of stability/silence). \n",
    "\n",
    "`compiam.melody.pattern.self_similarity` provides a method of computing the self similarity whilst skipping the computation for regions defined by a user specified exclusion mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2af1cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.melody.pattern import self_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe9d74",
   "metadata": {},
   "source": [
    "First let's compute the mask corresponding to regions we do not want to compute, these are regions of silence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a85b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_mask = pitch==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5f00b",
   "metadata": {},
   "source": [
    "And regions of consistent stability. To identify regions of consistent stability we use `compiam.utils.pitch.extract_stability_mask`. `extract_stability_mask` computes the average deviation of pitch from the mean in a series of windows across the track. If the deviation exceeds a certain threshold for a sufficient number of consecutive windows, the area is marked as stable (1 in the returned mask), else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9891c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.utils.pitch import extract_stability_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e5b65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_mask = extract_stability_mask(\n",
    "    pitch=pitch_cents, # pitch track\n",
    "    min_stab_sec=1.0, # minimum cummulative length of stable windows to warrant annotation\n",
    "    hop_sec=0.2, # hop length in seconds\n",
    "    var=60, # minimum variation from the mean in each window to be considered stable\n",
    "    timestep=timestep # time in seconds between consecutice elements in <pitch>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68b9b2",
   "metadata": {},
   "source": [
    "We can inspect the result to check it has worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c44216",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 304 # in seconds\n",
    "t2 = t1 + 10 # in seconds\n",
    "\n",
    "t1s = round(t1/timestep) # in sequence elements\n",
    "t2s = round(t2/timestep) # in sequence elements\n",
    "\n",
    "this_pitch = pitch[t1s:t2s]\n",
    "this_time = time[t1s:t2s]\n",
    "this_silence_mask = silence_mask[t1s:t2s]\n",
    "this_stable_mask = stability_mask[t1s:t2s]\n",
    "\n",
    "# get pitch plot\n",
    "fig, ax = plot_pitch(this_pitch, this_time, mask=this_silence_mask, tonic=tonic, yticks_dict=svara_pitch, cents=True, title='Excerpt of Koti Jamani by The Akkarai Sisters')\n",
    "\n",
    "# On alternative axis plot stable mask values\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(this_time, this_stable_mask, 'g', linewidth=1, alpha=1, linestyle='--')\n",
    "ax2.set_yticks([0,1])\n",
    "ax2.set_ylabel(\"Is stable region?\")\n",
    "    \n",
    "# accompanying audio\n",
    "ipy_audio(audio, t1, t2, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4834c762",
   "metadata": {},
   "source": [
    "Here the green line is to be read from the right-hand y-axis... is the region stable or not?\n",
    "\n",
    "Looks good! Let's combine the two masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfbda5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "exclusion_mask = np.logical_or(silence_mask==1, stability_mask==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30c1a3",
   "metadata": {},
   "source": [
    "And compute the self similarity `self_similarity()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "973c4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask of regions not interested in cite kaustuv, the papers that use \n",
    "ss = self_similarity(\n",
    "    ampl, # features\n",
    "    exclusion_mask=exclusion_mask, # exclusion mask\n",
    "    timestep=timestep, # time in seconds between elements of exlcusion mask\n",
    "    hop_length=cae.hop_length, # window size in audio frames\n",
    "    sr=cae.sr # sample rate of audio\n",
    ")\n",
    "\n",
    "# Sparsely computed self similarity matrix \n",
    "X = ss[0]\n",
    "# Mapping of index between theoretical full matrix and sparse one\n",
    "orig_sparse_lookup = ss[1]\n",
    "# Mapping of index between sparse matrix and theoretical full matrix one\n",
    "sparse_orig_lookup = ss[2]\n",
    "# Indices of boundaries between split regions in full matrix\n",
    "boundaries_orig = ss[3]\n",
    "# Indices of boundaries between split regions in sparse matrix\n",
    "boundaries_sparse = ss[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035816b",
   "metadata": {},
   "source": [
    "How does the self similarity matrix look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adebad28-1f49-434e-8640-067af0264056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.title(f'Self similarity matrix for Koti Janmani', fontsize=9)\n",
    "ax.imshow(X[2000:5000,2000:5000], interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ade44",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69525e9c",
   "metadata": {},
   "source": [
    "### 3.3 Segment extraction\n",
    "Diagonal segments in this self similarity matrix correspond to two regions of consistent high similarity (one on the x axis and y axis). We want to extract and define these segments.\n",
    "\n",
    "Instances of the same sañcāra are often performed slightly differently. Tempo differences mean that the segments cannot be expected to be parallel to the y=x diagonal. Differences in ornamention or differences in elaboration (such as the insertion of additional svaras or gamakas) mean that segments are sometimes not completely consistent unbroken lines, terminating and ending at exactly the same place.\n",
    "\n",
    "To deal with extraction in this context we use the segment extractor at `compiam.melody.pattern.segmentExtractor` which is adapted from {cite}`nuttall_patterns_2022b` and built specifically for the Carnatic Music context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a8bb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.melody.pattern import segmentExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8762b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emphasize Diagonal\n",
    "se = segmentExtractor(\n",
    "    X, # self sim matrix\n",
    "    window_size=cae.hop_length, # window size\n",
    "    cache_dir='.cache/' # cache directory for faster computation in future\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5c957",
   "metadata": {},
   "source": [
    "First we emphasize existing segments using a series of image processing techiniques:\n",
    "- Convolution with a sobel filter to emphasize edges\n",
    "- Normalise similarities to between 1 and 0\n",
    "- Binarize matrix to 0/1 above or below some threshold\n",
    "- Since the emphasized edges correspond to the borders of the segment and not the segment itself we apply morphological closing to fill gaps\n",
    "- Finally we apply morphological closing to smooth and remove noisy artifacts\n",
    "\n",
    "<div>\n",
    "<figure>\n",
    "<img src=\"../images/emphasizing_diagonals.png\" width=\"500\"/>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc = se.emphasize_diagonals()\n",
    "se.display_matrix(X_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d86a2",
   "metadata": {},
   "source": [
    "We can tune parameters visually using the `se.display_matrix` to view the processed plot. One of the most important parameters is the binary threshold, `bin_thresh`. Let's iterate through a sample and choose one that balances noise and segment integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cb720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0.05, 0.15, 0.01):\n",
    "    X_proc = se.emphasize_diagonals(bin_thresh=i)\n",
    "    se.display_matrix(X_proc[2000:5000,2000:5000], title=f'bin_thresh={round(i,2)}', figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88f5ac",
   "metadata": {},
   "source": [
    "We are looking for a solution that reduces the \"speccy\" noise parts and leaves strong, consistent diagonal segments.\n",
    "0.1 seems like an OK choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36b24973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc = se.emphasize_diagonals(bin_thresh=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4deefe",
   "metadata": {},
   "source": [
    "Finally we extract segments using a combination of geometric techniques:\n",
    "- Identify individual regions of non 0 values using a two-pass binary connected-components labelling algorithm (CCL)\n",
    "- Identify centroid of region\n",
    "- Idenitfy quadrant centroids\n",
    "- Define path through these centroids using least squares\n",
    "- Terminate path at bounding box of segment\n",
    "\n",
    "<div>\n",
    "<figure>\n",
    "<img src=\"../images/extracting_segments.png\" width=\"500\"/>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "**Note**: All identified segments are compared to each other, as such this step may take some time to run for long performances with many segments/repetitions. The intermediate processed data is cached in the above cache directory and hence for the same parameters should run immediately in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f16137",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "all_segments = se.extract_segments(\n",
    "    timestep=timestep, # timestep between\n",
    "    boundaries=boundaries_sparse, # boundaries of sparse regions (for conversion)\n",
    "    lookup=sparse_orig_lookup, # To convert between sparse and true indices\n",
    "    break_mask=exclusion_mask) # mask corresponding to break points, any segment that traverse these points are broken into two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3238d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Format: [(x0, y0), (x1, y1)]...\")\n",
    "all_segments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804b1d8",
   "metadata": {},
   "source": [
    "<a id='4.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459997d",
   "metadata": {},
   "source": [
    "### 3.4 Segment grouping\n",
    "Each segment corresponds to two regions of the audio, one on the x axis and one on the y axis. We want to convert these segments to start and end points of individual patterns and group these patterns into groups of occurrences of the same pattern. \n",
    "\n",
    "`segmentExtractor` has a grouping method which achieves this using a combination of the gemoetry of X and pairwise distances between patterns. First segments are grouped based on their alignment in the x and y direction. Subsequently, groups are merged together based on the average dynamic-time-warping distance between them. It is worth noting that DTW grouping is quite resource intensive, you can optionally turn it off by passing the parameter `thresh_dtw=None` to group_segments.\n",
    "\n",
    "A mask corresponding to anchor points to which we might want to extent returned patterns to can be passed to `segmentExtractor.group_segments` and any patterns that end or begin close to these points will be extended to them. Since regions of silence and stability serve as plasusible segmentation points between patterns we pass a mask indicating where the centers are for these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b384e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.utils import add_center_to_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36cad69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_mask_center = add_center_to_mask(exclusion_mask) # center of masked regions is annotated as \"2\"\n",
    "anchor_mask = np.array([1 if i==2 else 0 for i in exclusion_mask_center])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3e35f",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Returns patterns in units of pitch sequence elements\n",
    "starts_seq, lengths_seq = se.group_segments(\n",
    "    all_segments, # segments from se.extract_segments()\n",
    "    anchor_mask, # Extend patterns to these points\n",
    "    pitch, # pitch track\n",
    "    min_pattern_length_seconds=2, # minimum pattern length,\n",
    "    thresh_dtw=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a94b79",
   "metadata": {},
   "source": [
    "The process returns groups of patterns in terms of start point and length. The units for these patterns correspond to elements in the pitch track. We can convert to seconds as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b120cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_sec = [[x*timestep for x in p] for p in starts_seq]\n",
    "lengths_sec = [[x*timestep for x in l] for l in lengths_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25821220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of groups: {len(starts_sec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfe070",
   "metadata": {},
   "source": [
    "<a id='4.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd668630",
   "metadata": {},
   "source": [
    "### 3.5 Exploring results\n",
    "Let's take a look at some results. `compiam.visualisation.pitch.plot_subsequence` is a wrapper for`compiam.visualisation.pitch.plot_pitch` that allows us to inspect subsequences of a pitch plot with their surrounding melodic context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d6ba9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.visualisation.pitch import plot_subsequence\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2294b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs for plot_pitch\n",
    "plot_kwargs = {\n",
    "    'yticks_dict': svara_pitch,\n",
    "    'cents':True,\n",
    "    'tonic':tonic,\n",
    "    'figsize':(15,4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc11c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6 # Choose pattern group\n",
    "\n",
    "S = starts_seq[i] # get group\n",
    "L = lengths_seq[i] # get lengths\n",
    "\n",
    "for j,s in enumerate(S):\n",
    "    l = L[j] # this pattern length\n",
    "    ss = starts_sec[i][j] # this pattern start in seconds\n",
    "    ls = lengths_sec[i][j] # this pattern length in seconds\n",
    "    IPython.display.display(ipy_audio(audio, ss, ss+ls, sr=sr)) # display audio\n",
    "    # display pitch plot\n",
    "    plot_subsequence(s, l, pitch, time, timestep, path=None, plot_kwargs=plot_kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b935b9",
   "metadata": {},
   "source": [
    "<a id='4.6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2a0da",
   "metadata": {},
   "source": [
    "<a id='4.8'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
