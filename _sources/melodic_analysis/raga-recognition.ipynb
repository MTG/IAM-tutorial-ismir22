{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(raga-recognition)=\n",
    "# Automatic raga recognition\n",
    "\n",
    "Automatic [raga](carnatic-raga) recognition works intend to identify the raga that is performed in an Indian Art Music recording. Tradition-specific crafted features {cite}`gulati_patterns_2016_2` or distributional representation of raga grammar {cite}`ganguli_raga_2018` are used to identify and relate ragas. DL is also used to automatically recognize ragas on top of pitch curves using Long-Short Memory\n",
    "Time (LSTM) networks, given its ability to capture sequence information {cite}`madhusudhan_raga_2019`. In a recently published paper – included in the list of ISMIR 2022 accepted papers – the authors propose a novel approach for classifying ragas in a multimodal scenario using pitch curves and tonic (audio domain), and hand movements (video domain) {cite}`clayton_raga_2022`.\n",
    "\n",
    "Most of the research on this topic builds on top of the Dunya Carnatic and Hindustani corpora {cite}`iam_corpora`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Installing (if not) and importing compiam to the project\n",
    "import importlib.util\n",
    "if importlib.util.find_spec('compiam') is None:\n",
    "    ## Bear in mind this will only run in a jupyter notebook / Collab session\n",
    "    %pip install compiam\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first list the available tools for the task of raga recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiam.melody.raga_recognition.list_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(deepsrgm)=\n",
    "## DEEPSRGM\n",
    "\n",
    "We already see that the available tool appears with ``*`` at the end of the name, which indicates that it may be loaded using the ``compiam.load_models()`` wrapper. Observing [the entry of this tool in the `compiam` documentation](https://mtg.github.io/compIAM/source/melody.html#module-compiam.melody.raga_recognition.deepsrgm), we note that an optional dependency is needed to properly load this tool. Let's install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install torch==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEEPSRGM {cite}`madhusudhan_raga_2019` is a DL-based model that uses melodic features to automatically identify the raga from an Indian Art Music recording. In the original paper, the authors use the Indian Art Music Raga Dataset {cite}`raga_dataset`, and report accuracy higher than 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tool has * at the end: we can load the pre-trained instance\n",
    "deepsrgm = compiam.load_model(\"melody:deepsrgm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Deep Learning model checkpoints tend to be large in size, therefore storing these in `compiam` may become unsustainable. For that reason, we store the checkpoints in the cloud and download these when the user initializes a model using the wrapper. Note that you can specify to which location the checkpoint should be donwloaded by specifying `data_home` argument in `load_model()`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing raga mapping\n",
    "pprint(deepsrgm.mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the provided pre-trained model has been trained targetting to the ragas included in this mapping. Let's select some examples that are tagged with raga instances included in the default mapping to evaluate this tool and showcase how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize saraga Carnatic on our example audio folder\n",
    "saraga_carnatic = compiam.load_dataset(\n",
    "    \"saraga_carnatic\", data_home=\"./../audio/mir_datasets/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Please note that to save space, we are not loading the entire dataset but just a single recording. However, we can still use the dataset loader, which in this case will properly work as long as we only retrieve the data for this particular recording.\n",
    "```\n",
    "\n",
    "The recording we have included beforehand for tutoring purposes is *Sri Raghuvara Sugunaalaya*, performed by Sanjay Subrahmanyan, at Vani Mahal. We will load this track by first parsing its corresponding id in the `mirdata` dataloader (we do that beforehand). The said identifier is: ``109_Sri_Raghuvara_Sugunaalaya``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load the {ids: tracks} dict for Saraga Carnatic\n",
    "saraga_tracks = saraga_carnatic.load_tracks()\n",
    "\n",
    "# Now we can get a specific track\n",
    "track_data = saraga_tracks[\"109_Sri_Raghuvara_Sugunaalaya\"]\n",
    "print(\"This recording includes raaga:\", track_data.metadata[\"raaga\"][0][\"name\"])\n",
    "print(\"This recordings has unique Dunya ID:\", track_data.metadata[\"raaga\"][0][\"uuid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice!** Raga Bhairavi is both in this recording and included in the mapping of DEEPSRGM, therefore considered during the training process. We first need to extract the features from an audio recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Computing features for the given recording\n",
    "feat = deepsrgm.get_features(track_data.audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pass the computed features through the pre-trained model to perform inference and try to automatically recognise the raga performed in the recording.\n",
    "\n",
    "```python\n",
    "deepsrgm.predict(feat)\n",
    "```\n",
    "\n",
    "**Oops! Why is this line of code not being executed?** This ``.predict()`` method runs inference with the DEEPSRGM model using the passed features. As you may now, DL models tend to occupy an important amount of memory from your computer to run, especially in the training stage. Some light-weight models can be run from a conventional laptop, but in some other cases you might need a machine with enough power to run your models and perform inference. \n",
    "\n",
    "```{tip}\n",
    "As mentioned in the [introduction](google-collab), Google Collab has GPU and TPU access which may allow you running certain models that are too big for your machine.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THIS NOTEBOOK INTO A GOOGLE COLLAB SESSION\n",
    "## THEN, UNCOMMENT AND RUN THIS LINE:\n",
    "\n",
    "# deepsrgm.predict(feat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
