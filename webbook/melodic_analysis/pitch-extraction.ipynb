{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(melody-extraction)=\n",
    "# Pitch extraction\n",
    "\n",
    "As seen in the melodic introduction of [Carnatic](carnatic-melodic-concepts) and [Hindustani](hindustani-raga), pitch time-series is a very relevant feature to tackle the melodic analysis of these music traditions, showing utility for many musicologically-relevant problems. Given the shortage of recordings for isolated predominant melodic instruments, this task has had to be tackled directly from mixture recordings. In this context, the task is referred as *predominant* pitch extraction (otherwise commonly referred as *vocal* pitch extraction when the source is the singing voice).\n",
    "\n",
    "Historically, in the literature of computational analysis of Indian Art Music, this task has been tackled using knowledge-based approaches {cite}`rao_pitch_2010, salamon_pitch_2012`. Melodia {cite}`salamon_pitch_2012`, typically combined with additional post-processing steps {cite}`gulati_patterns_2016`, has been the preferred approach until today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Installing (if not) and importing compiam to the project\n",
    "import importlib.util\n",
    "if importlib.util.find_spec('compiam') is None:\n",
    "    ## Bear in mind this will only run in a jupyter notebook / Collab session\n",
    "    %pip install compiam\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first print out the available tools we do have available to extract the pitch from Indian Art Music recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(compiam.melody.pitch_extraction.list_tools())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Melodia\n",
    "\n",
    "Let's extract the pitch from an audio sample using Melodia {cite}`salamon_pitch_2012`. This is a salience-based method, meaning that it captures the most salient melodic segments in the time-frequency representation of the input signal, keeping and connecting the segments that are more likely to belong to the predominant melody, using heuristic rules. Make sure you check out the paper for further detail!\n",
    "\n",
    "We first need to install `essentia`, which is the optional dependency required to load this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install essentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Importing and initializing a melodia instance\n",
    "from compiam.melody.pitch_extraction import Melodia\n",
    "melodia = Melodia()  \n",
    "\n",
    "# Running extraction for an example track\n",
    "melodia_pitch_track = melodia.extract(\n",
    "    os.path.join(\n",
    "        \"..\", \"audio\", \"mir_datasets\", \"saraga1.5_carnatic\",\n",
    "        \"Live at Vani Mahal by Sanjay Subrahmanyan\", \"Sri Raghuvara Sugunaalaya\",\n",
    "        \"Sanjay Subrahmanyan - Sri Raghuvara Sugunaalaya.mp3\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the output pitch:\", np.shape(melodia_pitch_track))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can infer from the shape the number of estimated pitch values, while the dimension 2 refers to ``(frequency values, time-stamps)``. Let us show the first 5 time-stamps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(melodia_pitch_track[:5, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print out the final 5 pitch values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(melodia_pitch_track[-5:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melodia has been found, in the original paper experiments and also in the [MIREX campaign](https://nema.lis.illinois.edu/nema_out/mirex2011/results/ame/indian08/sg1results.html), to decently work on Indian Art Music samples. However, recent DL-based models have claimed the state-of-the-art for the task of pitch extraction. \n",
    "\n",
    "\n",
    "## FTANet-Carnatic\n",
    "\n",
    "**Maybe we can use a Carnatic-trained version of one of these models to extract the pitch?** Let's now import a DL model that learns to automatically extract the predominant melody from audio recordings. We will use FTANet {cite}`yu_pitch_2021`. This is an attention-based network that laverages and fuses information from frequency and periodicity domains to capture the right pitch values for the predominant source. It learns to focus on a particular source, using an additional branch that helps reducing the false alarm rate (detecting pitch values that do not correspond to the source we target).\n",
    "\n",
    "To train this model we need a dataset that includes mixture recordings plus annotated pitch time-series for the source we aim at capturing the pitch from. This FTANet instance is trained with the [Saraga Carnatic Melody Synth (SMCS)](https://zenodo.org/record/5553925).\n",
    "\n",
    "In the documentation we observe that this model is based on `tensorflow`, therefore we must install this dependency before importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow==2.9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.melody.pitch_extraction import FTANetCarnatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} This is a non-trained instance!\n",
    "Directly importing a ML/DL-based tool from the corresponding task initialises a non-trained instance. However, if a ``*`` appears at the end of the tool name when running ``.list_tools()``, the pre-trained weights can be loaded using the ``compiam.load_model()`` wrapper.\n",
    "```\n",
    "\n",
    "In ths case, we are only interested in inference. Therefore, we might be able to load FTANet as an already trained model. For that, let's print the models with available weights to load in `compiam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(compiam.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cool! A Carnatic-trained FTANet is there.** Therefore, let's load it and run inference on an example track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling tensorflow warnings and debugging info\n",
    "import os \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "\n",
    "# Importing tensorflow and disabling GPU usage\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first deactivate the GPU usage, since we assume no CUDA-capable GPU is available in most of the cases. We import `tensorflow` and set the visible GPU devices to none. We also disable the `tensorflow` warnings in order to keep the tutorial clean.\n",
    "\n",
    "```{note}\n",
    "If you have an available GPU to allocate the model, get the index of the GPU (probably 0 if you have only a single instance) and change ``tf.config.set_visible_devices([], \"GPU\")`` for ``os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"``\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing an FTANet instance\n",
    "ftanet_carnatic = compiam.load_model(\"melody:ftanet-carnatic\")\n",
    "\n",
    "# Predict!\n",
    "ftanet_pitch_track = ftanet_carnatic.predict(\n",
    "    os.path.join(\n",
    "        \"..\", \"audio\", \"mir_datasets\", \"saraga1.5_carnatic\",\n",
    "        \"Live at Vani Mahal by Sanjay Subrahmanyan\", \"Sri Raghuvara Sugunaalaya\",\n",
    "        \"Sanjay Subrahmanyan - Sri Raghuvara Sugunaalaya.mp3\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the extracted pitch tracks on top of the spectrogram of the input signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y, sr = librosa.load(\n",
    "    os.path.join(\n",
    "        \"..\", \"audio\", \"mir_datasets\", \"saraga1.5_carnatic\",\n",
    "        \"Live at Vani Mahal by Sanjay Subrahmanyan\", \"Sri Raghuvara Sugunaalaya\",\n",
    "        \"Sanjay Subrahmanyan - Sri Raghuvara Sugunaalaya.mp3\"\n",
    "    )\n",
    ")\n",
    "y = y[:3*sr]  # Getting just the first 3 seconds for better visualizatiom\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(15, 12))\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax);\n",
    "ax.set_ylim(0, 2000)\n",
    "plt.plot(\n",
    "    melodia_pitch_track[:, 0], melodia_pitch_track[:, 1],\n",
    "    color=\"white\", label=\"Melodia\",\n",
    ")\n",
    "plt.plot(\n",
    "    ftanet_pitch_track[:, 0], ftanet_pitch_track[:, 1],\n",
    "    color=\"black\",label=\"FTANet-Carnatic\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some final thoughts\n",
    "\n",
    "Again, going through the melodic introduction of [Carnatic](carnatic-melodic-concepts) and Hindustani Music, we note the importance of a reliable pitch extraction for the computational analysis of Indian Art Music. The task of pitch extraction has received quite a lot of attention recently and the state-of-the-art has been continuously moved forward, especially given the use of DL techniques. However, to the best of our knowledge, no DL-based pitch extraction has been trained or evaluated on Indian Art Music signals, the lack of data being the principal cause. The FTA-Net trained for Carnatic Music that we present in this walkthrough and that we have included in `compiam` has been trained with the [Saraga Carnatic Melody Synth (SMCS)](https://zenodo.org/record/5553925), a recently released dataset that includes several hours of Carnatic Music recordings annotated with ground-truth vocal pitch data that have been artificially compiled. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
