{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(datasets)=\n",
    "# Datasets\n",
    "\n",
    "Datasets are key for the data-driven computational research of a music tradition. Thoroughly designed collections of data that represent the most relevant aspects of a musical repertoire may open the door for solutions of several problems. For that reason, huge efforts have been made within the scope of this tutorial and ``compiam`` to (1) boost the visibility and access to Carnatic and Hindustani music datasets, and (2) provide standardized tools to get and use the said datasets.\n",
    "\n",
    "## `mirdata`\n",
    "[`mirdata`](https://mirdata.readthedocs.io/en/stable/) is an open-source and pip-installable Python library that provides tools for working with common Music Information Retrieval (MIR) datasets {cite}`mirdata`. Given the crucial importance and relevance of such a software for data and corpus-driven research, we have done a great efforts to integrate several IAM-centered datasets into `mirdata`. To date, the following datasets can be found in the latest `mirdata` release:\n",
    "\n",
    "* Carnatic collection of Saraga {cite}`saraga`\n",
    "* Hindustani collection of Saraga {cite}`saraga`\n",
    "* Carnatic Varnam Dataset {cite}`carnatic_varnam`\n",
    "* Carnatic Music Rhythm {cite}`carnatic_rhythm_dataset`\n",
    "* Hindustani Music Rhythm {cite}`hindustani_rhythm_dataset`\n",
    "* Indian Art Music Raga Dataset {cite}`raga_dataset`\n",
    "* Mridangam Stroke Dataset {cite}`mridangam_stroke`\n",
    "* Four-Way Tabla Dataset (ISMIR 2021) {cite}`4way_tabla`\n",
    "\n",
    "`compiam` provides access to these datasets through the mirdata loaders. Make sure to check the [`mirdata` documentation](https://mirdata.readthedocs.io/en/stable/source/mirdata.html#dataset-loaders) to learn the functionalities of the loaders.\n",
    "\n",
    "```{note}\n",
    "The alias of the `mirdata` method ``mirdata.initialize()`` in our library is ``compiam.load_dataset()``. Use this wrapper to access the `mirdata` loaders for Indian Art Music datasets from `compiam`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install compiam\n",
    "\n",
    "# Import compiam\n",
    "import compiam\n",
    "\n",
    "# Supress warnings to keep the tutorial clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "mridangam_stroke = compiam.load_dataset(\"mridangam_stroke\")\n",
    "mridangam_stroke.download()\n",
    "mridangam_stroke.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's get a random track from the dataset!\n",
    "mridangam_stroke.choice_track()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Run ``compiam.list_datasets()`` to list the available datasets to use.\n",
    "```\n",
    "\n",
    "### Why mirdata loaders?\n",
    "Accessing the datasets through `mirdata` brings numerous advantages and provides a more standardized and easy integration of the said datasets into our pipelines. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Loading all tracks from the dataset\n",
    "mridangam_tracks = mridangam_stroke.load_tracks()\n",
    "\n",
    "## Get available ragas\n",
    "available_strokes = np.unique([mridangam_tracks[x].stroke_name \\\n",
    "    for x in mridangam_stroke.track_ids])\n",
    "available_strokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mirdata` loaders help on getting the data loaded and organized without the need of writing functions to do that ourselves. In this example below, we create a dictionary in which stroke names are keys and for each key we have a list of audio samples including their respective stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_dict = {item: [] for item in available_strokes}\n",
    "for i in mridangam_stroke.track_ids:\n",
    "    stroke_dict[mridangam_tracks[i].stroke_name].append(mridangam_tracks[i].audio_path)\n",
    "\n",
    "stroke_dict['bheem'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's play this example!** Audio (and also annotations!) can be easily loaded from each track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first get a random track\n",
    "random_track = mridangam_stroke.choice_track()\n",
    "\n",
    "import IPython\n",
    "print(\"Play recording of id: {}, including stroke '{}' and tonic {}\"\\\n",
    "    .format(random_track.track_id, random_track.stroke_name, random_track.tonic))\n",
    "IPython.display.Audio(random_track.audio[0], rate=random_track.audio[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
