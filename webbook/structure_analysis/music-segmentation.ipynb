{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(music-segmentation)=\n",
    "# Music segmentation\n",
    "We have seen in the musicological introduction that we may come across different formats of [Carnatic](carnatic-formats) and Hindustani performances. These must be taken very much into account when designing strategies to segment the different sections in these musical pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Importing compiam to the project\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first list the available tools for music segmentation in `compiam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiam.structure.segmentation.list_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dhrupad Bandish segmentation\n",
    "\n",
    "In this section we will showcase a tool that attempts to identify, through the use of rhythmic features, the different sections in a Dhrupad Bandish performances {cite}`rohit_2020`, one of the main formats in Hindustani music. As seen in the [documentation](https://mtg.github.io/compIAM/source/structure.html#dhrupad-bandish-segmentation), this segmentation model is based on PyTorch. Therefore, we proceed to install ``torch``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install torch==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool may be accessed from the ``structure.segmentation``, however, the tool name has an ``*`` appended, therefore we can use the wrapper for models to rapidly initialize it with the pre-trained weights loaded.\n",
    "\n",
    "```{tip}\n",
    "Get the correct code for the wrapper by running ``compiam.list_models()``.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = compiam.load_model(\"structure:dhrupad-bandish-segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the documentation we observe that this model includes quite a number of attributes, and particularly we observe two of them that are interesting:\n",
    "* ``mode``\n",
    "* ``fold``\n",
    "\n",
    "These attributes are important because define the training pipeline that has been used and therefore, a different mode of operating with this model. ``mode`` has options: *net*, *voc*, or *pakh*, which indicate the source for  surface tempo multiple (s.t.m.) estimation. *net* mode is for input mixture signal, *voc* is for clean or source-separated singing voice recordings, and *pakh* for pakhawaj tracks (pakhawaj is a percussion instrument from Northern India). ``fold`` is basically an integer indicating with validation fold we do consider for training.\n",
    "\n",
    "These configuration variables are loaded by default as ``net`` and ``0`` respectively, however these may be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs.update_mode(mode=\"voc\")\n",
    "dbs.update_fold(fold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, the ``mode`` and ``fold`` have been updated and consequently, the class has automatically loaded the model weights corresponding to ``mode=voc`` and ``fold=1``.\n",
    "\n",
    "```{note}\n",
    "Typically in `compiam`, importing a model from the corresponding module or initializing it using the wrapper, can make an important difference on how the loaded instance works. Generally speaking, if you use the wrapper you will probably be only interested in running inference. If your goal is to train or deep-dive into a particular model, you should avoid the use of the model wrapper and start from a clean model instance.\n",
    "```\n",
    "\n",
    "Let's now run prediction on an input file. Our mode now is ``voc``, therefore the model expects a clean or source separated vocal signal. Isolated singing voice signals are not commonly available for the case of Carnatic and Hindustani music. We will use a state-of-the-art and out-of-the-box model, [`spleeter`](https://github.com/deezer/spleeter), to try to separate the singing voice from the accompaniment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install spleeter\n",
    "%pip install numba --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now directly download the pre-trained models for `spleeter`, and use these for inference in this walkthrough. We will use ``wget`` (UNIX-based) to download the available pre-trained weights for `spleeter` online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use `tarfile` to uncompress the downloaded file into a desired location. We will uncompres the downloaded model weights to the default location where `spleeter` looks for the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Open file\n",
    "file = tarfile.open(\"2stems.tar.gz\")\n",
    "\n",
    "# Creating directory where spleeter looks for models by default\n",
    "os.mkdir(\"pretrained_models/\")\n",
    "\n",
    "# Extracting files in tar\n",
    "file.extractall(\n",
    "    os.path.join(\"pretrained_models\", \"2stems\")\n",
    ")\n",
    "\n",
    "# Closing file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spleeter` is based on `TensorFlow`. We disable the GPU usage and the `TensorFlow` related warnings just like we did in the [pitch extraction walkthrough](melody-extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling tensorflow warnings and debugging info\n",
    "import os \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "\n",
    "# Importing tensorflow and disabling GPU usage\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now load the `spleeter` separator, which will automatically load the pre-trained weights for the model. We will use the ``2:stems`` model, which has been trained to separate *vocals* and *accompaniment*.\n",
    "\n",
    "```{note}\n",
    "The other option, which is the ``4:stems``, separates *vocals*, *bass*, *drums*, and *other*. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spleeter.separator import Separator\n",
    "\n",
    "# Load default 2-stem spleeter separation\n",
    "separator = Separator(\"spleeter:2stems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Separator` class in `spleeter` has a method to directly separate the singing voice from an audio file, and the prediction is stored in a given output folder. Let's use this method and get a source separated version of an example Dhrupad file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(compiam.list_datasets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops...** We note that no Dhrupad Bandish dataset is available in `mirdata`. Therefore, we will need to refer to the Carnatic and Hindustani corpora in Dunya. Let's get a couple of audio examples from Dunya using the `Corpora` class. As already mentioned before, you need a personal and non-shareable token to access the data in Dunya. Within the context of this tutorial, we provide here a snippet of code that we have used beforehand to parse the audios from the Dunya database.\n",
    "\n",
    "If we the folder `compiam/models/structure/dhrupad_bandish_segmentation/audio_original` within the installable `compiam`. We can see a .pdf file including the details of the files that form the dataset for the Dhrupad Segmentation tool. That .pdf file is also found in the [original repository of the tool](https://github.com/DAP-Lab/dhrupad-bandish-segmentation/blob/master/annotations/dataset_sources.pdf). We select the following example:\n",
    "\n",
    "* [59c88c32-0bde-433b-b194-0f65281e5714](https://musicbrainz.org/recording/59c88c32-0bde-433b-b194-0f65281e5714)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND RUN THIS CODE WITH YOUR PERSONAL TOKEN\n",
    "\n",
    "#from compiam import load_corpora\n",
    "#corpora = load_corpora(\n",
    "#    \"hindustani\",\n",
    "#    cc=False,  # Indicating we import de private collection\n",
    "#    token=\"<your-access-token>\",\n",
    "#\n",
    "#corpora.download_mp3(\n",
    "#    \"59c88c32-0bde-433b-b194-0f65281e5714\",\n",
    "#    \"os.path.join(\"..\", \"audio\")\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the audios are actually downloaded in the audio folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cool! There it is.** Let's therefore now run the `spleeter` separation on this track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separating file\n",
    "separator.separate_to_file(\n",
    "    os.path.join(\n",
    "        \"..\", \"audio\", \"59c88c32-0bde-433b-b194-0f65281e5714.mp3\"\n",
    "    ),\n",
    "    os.path.join(\"..\", \"audio\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separation done!** We can now run inference with the segmentation model on the source separated signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs.predict_stm(\n",
    "    file_path=os.path.join(\n",
    "        \"..\", \"audio\", \"59c88c32-0bde-433b-b194-0f65281e5714\", \"vocals.wav\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the estimated sections (given rhythmic characteristics) in the output image. The ``x`` axis provides information about the actual time-stamps in seconds for each estimation.\n",
    "\n",
    "As a final experiment, let's listen to the source separated file using `spleeter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "\n",
    "vocals, sr = librosa.load(\n",
    "    os.path.join(\n",
    "        \"..\", \"audio\", \"59c88c32-0bde-433b-b194-0f65281e5714\", \"vocals.wav\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "accompaniment, sr = librosa.load(\n",
    "    os.path.join(\n",
    "        \"..\", \"audio\", \"59c88c32-0bde-433b-b194-0f65281e5714\", \"accompaniment.wav\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\n",
    "    data=vocals[-sr*30:],  # Taking only the last 30 seconds\n",
    "    rate=sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\n",
    "    data=accompaniment[-sr*30:],  # Taking only the last 30 seconds\n",
    "    rate=sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iam-source-sep)=\n",
    "## A note on source separation for Indian Art Music\n",
    "As you may have noticed in the displayed audio above, even though `spleeter` is within the best out-of-the-box source separation models to use, we need to take into account some considerations in regards to the task of music source separation for Indian Art Music signals.\n",
    "\n",
    "First of all, although `spleeter` is trained with a massive amount of recordings, we can safely assume that Carnatic and Hindustani music do not have a considerable representation in the training set (this applies to the other out-of-the-box source separation models out there). In that sense, it is expected that these models struggle to generalize to Indian Art Music specific instruments and arrangements, which may cause abnormally low interference removal performance. The predominance of melodic monophonic accompaniment instruments in both [Carnatic](carnatic-instrumentation) (the violin being the most common case) and [Hindustani](hindustani-instrumentation) (harmonium in this case), the [tambura drone](carnatic-tambura-drone), the pitched percussion... These are high-level examples of elements that may cause the standardized source separation models to not generalize properly to Indian Art Music signals.\n",
    "\n",
    "What is more, the standardized source separation models target whether *vocals* and *accompaniment*, or *vocals*, *bass*, *drums*, and *other*. While to separate the singing voice from an accompaniment is OK, the *4 stem* configuration is far from being representative of the actual Carnatic and Hindustani Music arrangements.\n",
    "\n",
    "As a final note, another factor that is currently blocking the research on music source separation for Indian Art Music is the shortage of available datasets for this task. We have observed that the Saraga Carnatic collection has multi-track audio, but this has leakage (it has been recorded in live performances). In such case, a leakage-aware approach would be needed to use this data. Alternatively, a music source separation dataset including completely isolated and aligned tracks, which to the best of our knowledge is unavailable as of now, would open the door the music source separation research on Indian Art Music."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
