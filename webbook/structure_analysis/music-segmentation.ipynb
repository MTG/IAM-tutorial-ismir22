{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(music-segmentation)=\n",
    "# Music segmentation\n",
    "We have seen in the musicological introduction that we may come across different formats of [Carnatic](carnatic-formats) and Hindustani performances. These must be taken very much into account when designing strategies to segment the different sections in these musical pieces. In this section we will showcase a tool that attempts to identify, through the use of rhythmic features, the different sections in a Dhrupad Bandish performances {cite}`rohit_2020`, one of the main formats in Hindustani music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Importing compiam to the project\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first list the available tools for music segmentation in `compiam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiam.structure.segmentation.list_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the [documentation](https://mtg.github.io/compIAM/source/structure.html#dhrupad-bandish-segmentation), this segmentation model is based on PyTorch. Therefore, we proceed to install ``torch``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install torch==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool may be accessed from the ``structure.segmentation``, however, the tool name has an ``*`` appended, therefore we can use the wrapper for models to rapidly initialize it with the pre-trained weights loaded.\n",
    "\n",
    "```{tip}\n",
    "Get the correct code for the wrapper by running ``compiam.list_models()``.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = compiam.load_model(\"structure:dhrupad-bandish-segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the documentation we observe that this model includes quite a number of attributes, and particularly we observe two of them that are interesting:\n",
    "* ``mode``\n",
    "* ``fold``\n",
    "\n",
    "These attributes are important because define the training pipeline that has been used and therefore, a different mode of operating with this model. ``mode`` has options: *net*, *voc*, or *pakh*, which indicate the source for  surface tempo multiple (s.t.m.) estimation. *net* mode is for input mixture signal, *voc* is for clean or source-separated singing voice recordings, and *pakh* for pakhawaj tracks (pakhawaj is a percussion instrument from Northern India). ``fold`` is basically an integer indicating with validation fold we do consider for training.\n",
    "\n",
    "These configuration variables are loaded by default as ``net`` and ``0`` respectively, however these may be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs.update_mode(mode=\"voc\")\n",
    "dbs.update_fold(fold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, the ``mode`` and ``fold`` have been updated and consequently, the class has automatically loaded the model weights corresponding to ``mode=voc`` and ``fold=1``.\n",
    "\n",
    "```{note}\n",
    "Typically in `compiam`, importing a model from the corresponding module or initializing it using the wrapper, can make an important difference on how the loaded instance works. Generally speaking, if you use the wrapper you will probably be only interested in running inference. If your goal is to train or deep-dive into a particular model, you should avoid the use of the model wrapper and start from a clean model instance.\n",
    "```\n",
    "\n",
    "Let's now run prediction on an input file. Our mode now is ``voc``, therefore the model expects a clean or source separated vocal signal. Let's use a source separated example of \"Sharanu Janakana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs.predict_stm(path_to_file=os.path.join(\"..\", \"audio\", \"sharanu_janakana.wav\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
