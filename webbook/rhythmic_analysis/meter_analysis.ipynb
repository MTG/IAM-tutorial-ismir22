{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(meter-analysis)=\n",
    "# Meter analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "## Importing compiam to the project\n",
    "import compiam\n",
    "\n",
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Carnatic and Hindustani Music Rhyhtm datasets\n",
    "That is a precise moment to introduce the (CompMusic) [Carnatic](https://zenodo.org/record/1264394) and [Hindustani](https://zenodo.org/record/1264742) Rhythm Datasets. These datasets, which include audio recordings, musically-relevant metadata, and beat and meter annotations, can be downloaded from Zenodo under request and used through the `mirdata` dataloaders available from [release 0.3.7](https://github.com/mir-dataset-loaders/mirdata/releases/tag/0.3.7).\n",
    "\n",
    "Let's initialise an instance of these datasets and browse through the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmr = compiam.load_dataset(\n",
    "    \"compmusic_carnatic_rhythm\",\n",
    "    data_home=os.path.join(\"../audio/mir_datasets\"),\n",
    "    version=\"full_dataset\"\n",
    ")\n",
    "cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For showcasing purposes, we include a single-track version of the (CompMusic) Carnatic Rhythm Dataset within the materials of this tutorial. This dataset is private, but can be requested and downloaded for research purposes.\n",
    "\n",
    "Reading through the dataset details we observe the available data for the tracks in the dataloader. We will load the tracks and select the specific track we have selected for tutoring purposes. Bear in mind that you can access the list of identifiers in the dataloader with the ``.track_ids`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = cmr.load_tracks()[\"10001\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the available annotations for this example track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``BeatData`` is an annotation type that is used in `mirata` to store information related to rhythmic beats. We will print the time-steps for the first 20 annotated beats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.times[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also observe that we have the actual magnitude of these annotations clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.time_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe the poisitons in the cycle (if available) that each of the beats occupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.beats.positions[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mirdata` annotations have been created aiming at providing standardized and useful data structures for MIR-related annotations. In this example we showcase the use of ``BeatData``, but many more annotation types are included in `mirdata`. Make sure to [check them out](https://mirdata.readthedocs.io/en/latest/source/mirdata.html#annotations)!.\n",
    "\n",
    "Let's now observe how the meter annotations looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Akshara pulse tracker\n",
    "\n",
    "We can now extract the beats (referred as akshara pulses within the context of Indian Art Music) from the input recording. Likewise the other extractors and models in `compiam` (if not indicated otherwise), the method for inference takes an audio path as input.\n",
    "\n",
    "Let's first listen to the audio example we are going to be using to showcase this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# Let's also listen to the input audio\n",
    "ipd.Audio(\n",
    "    data=track.audio[0],\n",
    "    rate=track.audio[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import the Akshara Pulse Tracker tool from ``compiam.rhythm.meter``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the tool\n",
    "from compiam.rhythm.meter import AksharaPulseTracker\n",
    "\n",
    "# Let's initialize an instance of the tool\n",
    "apt = AksharaPulseTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "predicted_beats = apt.extract(track.audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the first 20 beat estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_beats[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the ``compiam.visualisation`` to plot the input audio with the annotations on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiam.visualisation.audio import plot_waveform\n",
    "help(plot_waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first plot the audio waveform of the example track with the ground-truth annotations on top. We observe that the ``labels`` input variable in ``plot_waveforms`` needs to be a dict with the following format: ``{time-step: label}``, while our estimation is basically a list of pulses. Therefore, we first need to convert the prediction into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from BeatData to {time-step: value} dict for plotting\n",
    "ground_truth_beats = {\n",
    "    time_step: position for time_step, position in \\\n",
    "        zip(track.beats.times, track.beats.position)\n",
    "}\n",
    "\n",
    "# We plot the first 5 seconds\n",
    "plot_waveform(\n",
    "    path_to_audio=track.audio_path,\n",
    "    t1=0,\n",
    "    t2=5,\n",
    "    labels=ground_truth_beats,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take the predicted beats, convert these into a dictionary as well, and plot them on top of the waveform of the input signal again, in order to compare between the estimation and the ground-truth above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_beats_dict = {\n",
    "    time_step: idx for idx, time_step in enumerate(predicted_beats)\n",
    "}\n",
    "\n",
    "# And we plot!\n",
    "plot_waveform(\n",
    "    path_to_audio=track.audio_path,\n",
    "    t1=0,\n",
    "    t2=5,\n",
    "    labels=predicted_beats_dict,\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
